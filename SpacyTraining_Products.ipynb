{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import json\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRecognizer\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.tagger import Tagger\n",
    " \n",
    "try:\n",
    "    unicode\n",
    "except:\n",
    "    unicode = str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ner(nlp, train_data, entity_types):\n",
    "    # Add new words to vocab.\n",
    "    for raw_text, _ in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        for word in doc:\n",
    "            _ = nlp.vocab[word.orth]\n",
    "#             print(_)\n",
    "\n",
    "    # Train NER.\n",
    "    ner = EntityRecognizer(nlp.vocab, entity_types=entity_types)\n",
    "    for itn in range(5):\n",
    "        random.shuffle(train_data)\n",
    "        for raw_text, entity_offsets in train_data:\n",
    "            doc = nlp.make_doc(raw_text)\n",
    "            gold = GoldParse(doc, entities=entity_offsets)\n",
    "            ner.update(doc, gold)\n",
    "    return ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(ner, model_dir):\n",
    "  \n",
    "   \n",
    "    model_dir = pathlib.Path(model_dir)\n",
    "   \n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir()\n",
    "    assert model_dir.is_dir()\n",
    "\n",
    "    with (model_dir / 'config.json').open('wb') as file_:\n",
    "        data = json.dumps(ner.cfg)\n",
    "        if isinstance(data, unicode):\n",
    "            data = data.encode('utf8')\n",
    "        file_.write(data)\n",
    "    ner.model.dump(str(model_dir / 'model'))\n",
    "    if not (model_dir / 'vocab').exists():\n",
    "        (model_dir / 'vocab').mkdir()\n",
    "    ner.vocab.dump(str(model_dir / 'vocab' / 'lexemes.bin'))\n",
    "    with (model_dir / 'vocab' / 'strings.json').open('w', encoding='utf8') as file_:\n",
    "        ner.vocab.strings.dump(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(model_dir=None):\n",
    "    #, parser=False, entity=False, add_vectors=False)\n",
    "\n",
    "    # v1.1.2 onwards\n",
    "    if nlp.tagger is None:\n",
    "        print('---- WARNING ----')\n",
    "        print('Data directory not found')\n",
    "        print('please run: `python -m spacy.en.download --force all` for better performance')\n",
    "        print('Using feature templates for tagging')\n",
    "        print('-----------------')\n",
    "        nlp.tagger = Tagger(nlp.vocab, features=Tagger.feature_templates)\n",
    "    train_data = [\n",
    "        (\n",
    "            'I want to buy a Boxer',\n",
    "            [(len('I want to buy a '), len('I want to buy a Boxer'), 'PRODUCT')]\n",
    "        ),\n",
    "        (\n",
    "            'Do you have a Blanket',\n",
    "            [(len('Do you have a '), len('Do you have Blanket'), 'PRODUCT')]\n",
    "        )\n",
    "    ]\n",
    "    ner = train_ner(nlp, train_data, ['PRODUCT'])\n",
    "\n",
    "#     doc = nlp.make_doc('I want a Blanket')\n",
    "#     nlp.tagger(doc)\n",
    "#     ner(doc)\n",
    "#     for word in doc:\n",
    "#         print(word.text, word.orth, word.lower, word.tag_, word.ent_type_, word.ent_iob)\n",
    "#     train_data = [\n",
    "#         (\n",
    "#             'Radha',\n",
    "#             [(0, len('Radha'), 'PRODUCT')]\n",
    "#         )\n",
    "#         ]\n",
    "#     ner = train_ner(nlp, train_data, ['PRODUCT'])  \n",
    "#     doc = nlp.make_doc('where is London?')\n",
    "#     nlp.tagger(doc)\n",
    "#     ner(doc)\n",
    "#     for word in doc:\n",
    "#         print(word.text,word.ent_type_)\n",
    "    \n",
    "\n",
    "    if model_dir is not None:\n",
    "        save_model(ner, model_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictEnt(query):\n",
    "    nlp = spacy.load('en')\n",
    "    doc = nlp.make_doc(query)\n",
    "    vocab_dir = pathlib.Path('ner/vocab')\n",
    "    with (vocab_dir / 'strings.json').open('r', encoding='utf8') as file_:\n",
    "        nlp.vocab.strings.load(file_)\n",
    "    nlp.vocab.load_lexemes(vocab_dir / 'lexemes.bin')\n",
    "    ner = EntityRecognizer.load(pathlib.Path(\"ner\"), nlp.vocab, require=True)\n",
    "    nlp.tagger(doc)\n",
    "    ner(doc)\n",
    "    for word in doc:\n",
    "        if word.ent_type_=='PRODUCT':\n",
    "            return word.text\n",
    "        #print(word.text, word.orth, word.lower, word.tag_, word.ent_type_, word.ent_iob)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blanket\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    ent=predictEnt('can I buy a Blanket?')\n",
    "    print(ent)\n",
    "    # Who \"\" 2\n",
    "    # is \"\" 2\n",
    "    # Shaka \"\" PERSON 3\n",
    "    # Khan \"\" PERSON 1\n",
    "    # ? \"\" 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
